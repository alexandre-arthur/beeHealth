{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File to use the Machine Learning in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the other files in the project\n",
    "\n",
    "import ModelFunctions as model\n",
    "import HoneyDatasetCreator as datasetCreator\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the dataset for the ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CF003 - Active - Day - (215)\n",
      "CF003 - Active - Day - (215) is done in 0.3863394260406494s (1/52).\n",
      "Starting CF003 - Active - Day - (216)\n",
      "CF003 - Active - Day - (216) is done in 0.13704967498779297s (2/52).\n",
      "Starting CF003 - Active - Day - (217)\n",
      "CF003 - Active - Day - (217) is done in 0.3138148784637451s (3/52).\n",
      "Starting CF003 - Active - Day - (218)\n",
      "CF003 - Active - Day - (218) is done in 0.26001739501953125s (4/52).\n",
      "Starting CF003 - Active - Day - (219)\n",
      "CF003 - Active - Day - (219) is done in 0.13442754745483398s (5/52).\n",
      "Starting CF003 - Active - Day - (220)\n",
      "CF003 - Active - Day - (220) is done in 0.07958555221557617s (6/52).\n",
      "Starting CF003 - Active - Day - (221)\n",
      "CF003 - Active - Day - (221) is done in 0.14740371704101562s (7/52).\n",
      "Starting CF003 - Active - Day - (222)\n",
      "CF003 - Active - Day - (222) is done in 0.1030275821685791s (8/52).\n",
      "Starting CF003 - Active - Day - (223)\n",
      "CF003 - Active - Day - (223) is done in 0.09217166900634766s (9/52).\n",
      "Starting CF003 - Active - Day - (224)\n",
      "CF003 - Active - Day - (224) is done in 0.22335362434387207s (10/52).\n",
      "Starting CF003 - Active - Day - (225)\n",
      "CF003 - Active - Day - (225) is done in 0.11522603034973145s (11/52).\n",
      "Starting CF003 - Active - Day - (226)\n",
      "CF003 - Active - Day - (226) is done in 0.14680695533752441s (12/52).\n",
      "Starting CF003 - Active - Day - (227)\n",
      "CF003 - Active - Day - (227) is done in 0.2116250991821289s (13/52).\n",
      "Starting CJ001 - Missing Queen - Day -  (100)\n",
      "CJ001 - Missing Queen - Day -  (100) is done in 0.0s (14/52).\n",
      "Starting CJ001 - Missing Queen - Day -  (101)\n",
      "CJ001 - Missing Queen - Day -  (101) is done in 0.1201775074005127s (15/52).\n",
      "Starting CJ001 - Missing Queen - Day -  (102)\n",
      "CJ001 - Missing Queen - Day -  (102) is done in 0.16758084297180176s (16/52).\n",
      "Starting CJ001 - Missing Queen - Day -  (103)\n",
      "CJ001 - Missing Queen - Day -  (103) is done in 0.15083694458007812s (17/52).\n",
      "Starting CJ001 - Missing Queen - Day -  (104)\n",
      "CJ001 - Missing Queen - Day -  (104) is done in 0.0s (18/52).\n",
      "Starting Hive1_12_06_2018_QueenBee_H1_audio___15_00_00\n",
      "Hive1_12_06_2018_QueenBee_H1_audio___15_00_00 is done in 0.6194653511047363s (19/52).\n",
      "Starting Hive1_12_06_2018_QueenBee_H1_audio___15_10_00\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed to serialize the data, got [0.27635413 0.37483027 0.45615436 ... 0.31495198 0.523784   0.71020645] and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\garni\\Documents\\Oulu\\Stage\\beeHealth\\DeepLearning\\NeuralHivework\\HoneyDatasetCreator.py:251\u001b[0m, in \u001b[0;36mwriteToFileAsCSV\u001b[1;34m(outputPath, data, bee, sampleNumber)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m :\n\u001b[1;32m--> 251\u001b[0m     toWrite \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(value) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m data) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(bee) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m# Transform as CSV\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\garni\\Documents\\Oulu\\Stage\\beeHealth\\DeepLearning\\NeuralHivework\\HoneyDatasetCreator.py:251\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m :\n\u001b[1;32m--> 251\u001b[0m     toWrite \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(value) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m data) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(bee) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m# Transform as CSV\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m datasetName \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFastHoneyTransform_\u001b[39m\u001b[39m{\u001b[39;00msampleSeconds\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m sizeOfALine \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m55125\u001b[39m \u001b[39m/\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m sampleSeconds) \u001b[39m# Size of the line in the csv file\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m datasetCreator\u001b[39m.\u001b[39;49mstoreEverySampleButLast(audioPath, datasetPath, datasetName, sampleSeconds, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, timeVerbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     11\u001b[0m datasetName \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLastSamples_\u001b[39m\u001b[39m{\u001b[39;00msampleSeconds\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m datasetCreator\u001b[39m.\u001b[39mstoreLastSample(audioPath, datasetPath, datasetName, sampleSeconds, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, timeVerbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\garni\\Documents\\Oulu\\Stage\\beeHealth\\DeepLearning\\NeuralHivework\\HoneyDatasetCreator.py:55\u001b[0m, in \u001b[0;36mstoreEverySampleButLast\u001b[1;34m(audioPath, datasetPath, datasetName, sampleTime, timeVerbose, verbose)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStarting \u001b[39m\u001b[39m{\u001b[39;00mfileName\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 55\u001b[0m StoreFilesToCSV(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00maudioPath\u001b[39m}\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mfileName\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, datasetPath, datasetName, maxFreq\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m, sampleTime\u001b[39m=\u001b[39;49msampleTime)\n\u001b[0;32m     57\u001b[0m \u001b[39mif\u001b[39;00m timeVerbose :\n\u001b[0;32m     58\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfileName\u001b[39m}\u001b[39;00m\u001b[39m is done in \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mt\u001b[39m}\u001b[39;00m\u001b[39ms (\u001b[39m\u001b[39m{\u001b[39;00mcounter\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(fileNames[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\garni\\Documents\\Oulu\\Stage\\beeHealth\\DeepLearning\\NeuralHivework\\HoneyDatasetCreator.py:113\u001b[0m, in \u001b[0;36mStoreFilesToCSV\u001b[1;34m(genericAudioPath, datasetPath, modelFileName, maxFreq, sampleTime, timeVerbose, verbose)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39m# Write the data to the CSV file\u001b[39;00m\n\u001b[0;32m    112\u001b[0m sampleNumber \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m55125\u001b[39m \u001b[39m/\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m sampleTime)\n\u001b[1;32m--> 113\u001b[0m writeToFileAsCSV(datasetName, magnitudes, bee, sampleNumber\u001b[39m=\u001b[39;49msampleNumber)\n",
      "File \u001b[1;32mc:\\Users\\garni\\Documents\\Oulu\\Stage\\beeHealth\\DeepLearning\\NeuralHivework\\HoneyDatasetCreator.py:253\u001b[0m, in \u001b[0;36mwriteToFileAsCSV\u001b[1;34m(outputPath, data, bee, sampleNumber)\u001b[0m\n\u001b[0;32m    251\u001b[0m     toWrite \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(value) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m data) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(bee) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m# Transform as CSV\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m--> 253\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to serialize the data, got \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mbee\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    255\u001b[0m \u001b[39m# Write the data to the file\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(outputPath, \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n",
      "\u001b[1;31mException\u001b[0m: Failed to serialize the data, got [0.27635413 0.37483027 0.45615436 ... 0.31495198 0.523784   0.71020645] and 1"
     ]
    }
   ],
   "source": [
    "audioPath = os.path.join(\"../../Audio/BeeDataset/RawFilesFromWeb/\")\n",
    "datasetPath = \"HugeCSVHolder/\"\n",
    "sampleSeconds = 10 # Number of seconds we want to have in each sample for the ML\n",
    "\n",
    "\n",
    "datasetName = f\"FastHoneyTransform_{sampleSeconds}.csv\"\n",
    "sizeOfALine = int(55125 / 10 * sampleSeconds) # Size of the line in the csv file\n",
    "\n",
    "datasetCreator.storeEverySampleButLast(audioPath, datasetPath, datasetName, sampleSeconds, verbose=True, timeVerbose=True)\n",
    "\n",
    "datasetName = f\"LastSamples_{sampleSeconds}.csv\"\n",
    "datasetCreator.storeLastSample(audioPath, datasetPath, datasetName, sampleSeconds, verbose=True, timeVerbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to load the data, verify if the file HugeCSVHolder/FastHoneyTransform2.csv exists.\n"
     ]
    }
   ],
   "source": [
    "dataLength = sizeOfALine\n",
    "model.createAndStoreModelFromCSV(f\"HugeCSVHolder/FastHoneyTransform2.csv\", f\"ModelBeeHealth/model2\", [5000, 3000, 1000, 500, 500], dataLength=dataLength, verbose=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the model from the computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 55125 is out of bounds for axis 1 with size 27563",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X, Y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mloadCSVData(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHugeCSVHolder/LastSamples.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, dataLength\u001b[39m=\u001b[39;49mdataLength, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\garni\\Documents\\Oulu\\Stage\\beeHealth\\DeepLearning\\NeuralHivework\\ModelFunctions.py:30\u001b[0m, in \u001b[0;36mloadCSVData\u001b[1;34m(fileName, dataLength, verbose)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m# split into input (X : data) and output (Y : expected result) variables\u001b[39;00m\n\u001b[0;32m     29\u001b[0m X \u001b[39m=\u001b[39m dataset[:,\u001b[39m0\u001b[39m:dataLength]\n\u001b[1;32m---> 30\u001b[0m Y \u001b[39m=\u001b[39m dataset[:,dataLength]\n\u001b[0;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m X, Y\n",
      "\u001b[1;31mIndexError\u001b[0m: index 55125 is out of bounds for axis 1 with size 27563"
     ]
    }
   ],
   "source": [
    "X, Y = model.loadCSVData(f\"HugeCSVHolder/LastSamples.csv\", dataLength=dataLength, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m p, y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(model\u001b[39m.\u001b[39mloadModel(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModelBeeHealth/model2\u001b[39m\u001b[39m\"\u001b[39m), X, Y, timeVerbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m s \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m prediction, expected \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(p\u001b[39m.\u001b[39mtolist(), y\u001b[39m.\u001b[39mtolist()) :\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "p, y = model.predict(model.loadModel(f\"ModelBeeHealth/model2\"), X, Y, timeVerbose=True)\n",
    "\n",
    "s = 0\n",
    "for prediction, expected in zip(p.tolist(), y.tolist()) :\n",
    "    if(prediction[0] == (int) (expected)):\n",
    "        s += 1\n",
    "\n",
    "print(f\"Accuracy : {s / len(p) * 100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
