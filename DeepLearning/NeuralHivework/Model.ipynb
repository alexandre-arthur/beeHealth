{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"10\"> DOESN'T WORK FOR LINUX </font>\n",
    "  <font size=\"1\"> (Sorry) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File to use the Machine Learning in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garni\\Documents\\Oulu\\Stage\\BeeHealth\n"
     ]
    }
   ],
   "source": [
    "# Import the other files in the project\n",
    "\n",
    "import ModelFunctions as model\n",
    "import HoneyDatasetCreator as datasetCreator\n",
    "import os\n",
    "\n",
    "while \"Audio\" not in os.listdir():\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the dataset for the ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garni\\Documents\\Oulu\\Stage\\BeeHealth\n",
      "Starting CF003 - Active - Day - (215)\n",
      "CF003 - Active - Day - (215) is done in 0.34331369400024414s (1/52).\n",
      "Starting CF003 - Active - Day - (216)\n",
      "CF003 - Active - Day - (216) is done in 0.16742753982543945s (2/52).\n",
      "Starting CF003 - Active - Day - (217)\n",
      "CF003 - Active - Day - (217) is done in 0.3295884132385254s (3/52).\n",
      "Starting CF003 - Active - Day - (218)\n",
      "CF003 - Active - Day - (218) is done in 0.3069310188293457s (4/52).\n",
      "Starting CF003 - Active - Day - (219)\n",
      "CF003 - Active - Day - (219) is done in 0.1402721405029297s (5/52).\n",
      "Starting CF003 - Active - Day - (220)\n",
      "CF003 - Active - Day - (220) is done in 0.1098320484161377s (6/52).\n",
      "Starting CF003 - Active - Day - (221)\n",
      "CF003 - Active - Day - (221) is done in 0.18895673751831055s (7/52).\n",
      "Starting CF003 - Active - Day - (222)\n",
      "CF003 - Active - Day - (222) is done in 0.11403036117553711s (8/52).\n",
      "Starting CF003 - Active - Day - (223)\n",
      "CF003 - Active - Day - (223) is done in 0.10320591926574707s (9/52).\n",
      "Starting CF003 - Active - Day - (224)\n",
      "CF003 - Active - Day - (224) is done in 0.2027881145477295s (10/52).\n",
      "Starting CF003 - Active - Day - (225)\n",
      "CF003 - Active - Day - (225) is done in 0.09100151062011719s (11/52).\n",
      "Starting CF003 - Active - Day - (226)\n",
      "CF003 - Active - Day - (226) is done in 0.15277552604675293s (12/52).\n",
      "Starting CF003 - Active - Day - (227)\n",
      "CF003 - Active - Day - (227) is done in 0.2361159324645996s (13/52).\n",
      "Starting CJ001 - Missing Queen - Day -  (100)\n",
      "CJ001 - Missing Queen - Day -  (100) is done in 0.0010030269622802734s (14/52).\n",
      "Starting CJ001 - Missing Queen - Day -  (101)\n",
      "CJ001 - Missing Queen - Day -  (101) is done in 0.1331920623779297s (15/52).\n",
      "Starting CJ001 - Missing Queen - Day -  (102)\n",
      "CJ001 - Missing Queen - Day -  (102) is done in 0.2112579345703125s (16/52).\n",
      "Starting CJ001 - Missing Queen - Day -  (103)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m datasetName \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFastHoneyTransform_\u001b[39m\u001b[39m{\u001b[39;00msampleSeconds\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m sizeOfALine \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m55125\u001b[39m \u001b[39m/\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m sampleSeconds) \u001b[39m# Size of the line in the csv file\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m datasetCreator\u001b[39m.\u001b[39;49mstoreEverySampleButLast(audioPath, datasetPath, datasetName, sampleSeconds, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, timeVerbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     14\u001b[0m datasetName \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLastSamples_\u001b[39m\u001b[39m{\u001b[39;00msampleSeconds\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m datasetCreator\u001b[39m.\u001b[39mstoreLastSample(audioPath, datasetPath, datasetName, sampleSeconds, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, timeVerbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\garni\\Documents\\Oulu\\Stage\\beeHealth\\DeepLearning\\NeuralHivework\\HoneyDatasetCreator.py:55\u001b[0m, in \u001b[0;36mstoreEverySampleButLast\u001b[1;34m(audioPath, datasetPath, datasetName, sampleTime, timeVerbose, verbose)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStarting \u001b[39m\u001b[39m{\u001b[39;00mfileName\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 55\u001b[0m StoreFilesToCSV(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00maudioPath\u001b[39m}\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mfileName\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, datasetPath, datasetName, maxFreq\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m, sampleTime\u001b[39m=\u001b[39;49msampleTime)\n\u001b[0;32m     57\u001b[0m \u001b[39mif\u001b[39;00m timeVerbose :\n\u001b[0;32m     58\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfileName\u001b[39m}\u001b[39;00m\u001b[39m is done in \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mt\u001b[39m}\u001b[39;00m\u001b[39ms (\u001b[39m\u001b[39m{\u001b[39;00mcounter\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(fileNames[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\garni\\Documents\\Oulu\\Stage\\beeHealth\\DeepLearning\\NeuralHivework\\HoneyDatasetCreator.py:113\u001b[0m, in \u001b[0;36mStoreFilesToCSV\u001b[1;34m(genericAudioPath, datasetPath, modelFileName, maxFreq, sampleTime, timeVerbose, verbose)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39m# Write the data to the CSV file\u001b[39;00m\n\u001b[0;32m    112\u001b[0m sampleNumber \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m55125\u001b[39m \u001b[39m/\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m sampleTime)\n\u001b[1;32m--> 113\u001b[0m writeToFileAsCSV(datasetName, magnitudes, bee, sampleNumber\u001b[39m=\u001b[39;49msampleNumber)\n",
      "File \u001b[1;32mc:\\Users\\garni\\Documents\\Oulu\\Stage\\beeHealth\\DeepLearning\\NeuralHivework\\HoneyDatasetCreator.py:256\u001b[0m, in \u001b[0;36mwriteToFileAsCSV\u001b[1;34m(outputPath, data, bee, sampleNumber)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to serialize the data, got \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mbee\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    255\u001b[0m \u001b[39m# Write the data to the file\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(outputPath, \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m    257\u001b[0m         \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39msize \u001b[39m!=\u001b[39m sampleNumber: \u001b[39m# 55125 is the number of samples for 10 seconds\u001b[39;00m\n\u001b[0;32m    258\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWarning : Size of the data os \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m.\u001b[39msize\u001b[39m}\u001b[39;00m\u001b[39m, expected \u001b[39m\u001b[39m{\u001b[39;00msampleNumber\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m<frozen codecs>:186\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "audioPath = \"Audio/BeeDataset/RawFilesFromWeb/\"\n",
    "neuralNetworkPath = \"Deeplearning/NeuralHivework/\"\n",
    "datasetPath = f\"{neuralNetworkPath}HugeCSVHolder/\"\n",
    "\n",
    "print(os.getcwd())\n",
    "sampleSeconds = 10 # Number of seconds we want to have in each sample for the ML\n",
    "\n",
    "\n",
    "datasetName = f\"FastHoneyTransform_{sampleSeconds}.csv\"\n",
    "sizeOfALine = int(55125 / 10 * sampleSeconds) # Size of the line in the csv file\n",
    "\n",
    "datasetCreator.storeEverySampleButLast(audioPath, datasetPath, datasetName, sampleSeconds, verbose=True, timeVerbose=True)\n",
    "\n",
    "datasetName = f\"LastSamples_{sampleSeconds}.csv\"\n",
    "datasetCreator.storeLastSample(audioPath, datasetPath, datasetName, sampleSeconds, verbose=True, timeVerbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to load the data, verify if the file Deeplearning/NeuralHivework/HugeCSVHolder/FastHoneyTransform2.csv exists.\n"
     ]
    }
   ],
   "source": [
    "dataLength = sizeOfALine\n",
    "model.createAndStoreModelFromCSV(f\"{neuralNetworkPath}HugeCSVHolder/FastHoneyTransform2.csv\", f\"{neuralNetworkPath}ModelBeeHealth/model2\", [5000, 3000, 1000, 500, 500], dataLength=dataLength, verbose=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the model from the computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X, Y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mloadCSVData(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mneuralNetworkPath\u001b[39m}\u001b[39;49;00m\u001b[39mHugeCSVHolder/LastSamples.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, dataLength\u001b[39m=\u001b[39;49mdataLength, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\garni\\Documents\\Oulu\\Stage\\beeHealth\\DeepLearning\\NeuralHivework\\ModelFunctions.py:26\u001b[0m, in \u001b[0;36mloadCSVData\u001b[1;34m(fileName, dataLength, verbose)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mfileName must be a csv , file was : \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m fileName)\n\u001b[0;32m     25\u001b[0m \u001b[39m# load the datatset\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m dataset \u001b[39m=\u001b[39m loadtxt(fileName, delimiter\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     28\u001b[0m \u001b[39m# split into input (X : data) and output (Y : expected result) variables\u001b[39;00m\n\u001b[0;32m     29\u001b[0m X \u001b[39m=\u001b[39m dataset[:,\u001b[39m0\u001b[39m:dataLength]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\npyio.py:1356\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1353\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(delimiter, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1354\u001b[0m     delimiter \u001b[39m=\u001b[39m delimiter\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1356\u001b[0m arr \u001b[39m=\u001b[39m _read(fname, dtype\u001b[39m=\u001b[39;49mdtype, comment\u001b[39m=\u001b[39;49mcomment, delimiter\u001b[39m=\u001b[39;49mdelimiter,\n\u001b[0;32m   1357\u001b[0m             converters\u001b[39m=\u001b[39;49mconverters, skiplines\u001b[39m=\u001b[39;49mskiprows, usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[0;32m   1358\u001b[0m             unpack\u001b[39m=\u001b[39;49munpack, ndmin\u001b[39m=\u001b[39;49mndmin, encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   1359\u001b[0m             max_rows\u001b[39m=\u001b[39;49mmax_rows, quote\u001b[39m=\u001b[39;49mquotechar)\n\u001b[0;32m   1361\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\npyio.py:999\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m    996\u001b[0m     data \u001b[39m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[0;32m    998\u001b[0m \u001b[39mif\u001b[39;00m read_dtype_via_object_chunks \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 999\u001b[0m     arr \u001b[39m=\u001b[39m _load_from_filelike(\n\u001b[0;32m   1000\u001b[0m         data, delimiter\u001b[39m=\u001b[39;49mdelimiter, comment\u001b[39m=\u001b[39;49mcomment, quote\u001b[39m=\u001b[39;49mquote,\n\u001b[0;32m   1001\u001b[0m         imaginary_unit\u001b[39m=\u001b[39;49mimaginary_unit,\n\u001b[0;32m   1002\u001b[0m         usecols\u001b[39m=\u001b[39;49musecols, skiplines\u001b[39m=\u001b[39;49mskiplines, max_rows\u001b[39m=\u001b[39;49mmax_rows,\n\u001b[0;32m   1003\u001b[0m         converters\u001b[39m=\u001b[39;49mconverters, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1004\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding, filelike\u001b[39m=\u001b[39;49mfilelike,\n\u001b[0;32m   1005\u001b[0m         byte_converters\u001b[39m=\u001b[39;49mbyte_converters)\n\u001b[0;32m   1007\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1008\u001b[0m     \u001b[39m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m     \u001b[39m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m     \u001b[39m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[39m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m     \u001b[39mif\u001b[39;00m filelike:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\encodings\\cp1252.py:22\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mIncrementalDecoder\u001b[39;00m(codecs\u001b[39m.\u001b[39mIncrementalDecoder):\n\u001b[1;32m---> 22\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     23\u001b[0m         \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39mcharmap_decode(\u001b[39minput\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, Y = model.loadCSVData(f\"{neuralNetworkPath}HugeCSVHolder/LastSamples.csv\", dataLength=dataLength, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ModelBeeHealth/model2.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m p, y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(model\u001b[39m.\u001b[39;49mloadModel(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mModelBeeHealth/model2\u001b[39;49m\u001b[39m\"\u001b[39;49m), X, Y, timeVerbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m s \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m prediction, expected \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(p\u001b[39m.\u001b[39mtolist(), y\u001b[39m.\u001b[39mtolist()) :\n",
      "File \u001b[1;32mc:\\Users\\garni\\Documents\\Oulu\\Stage\\beeHealth\\DeepLearning\\NeuralHivework\\ModelFunctions.py:123\u001b[0m, in \u001b[0;36mloadModel\u001b[1;34m(fileName, verbose)\u001b[0m\n\u001b[0;32m    121\u001b[0m t \u001b[39m=\u001b[39m time()\n\u001b[0;32m    122\u001b[0m \u001b[39m# load json and create model\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m loadedModel \u001b[39m=\u001b[39m LoadJSONModel(fileName, verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[0;32m    125\u001b[0m \u001b[39m# evaluate loaded model on test data\u001b[39;00m\n\u001b[0;32m    126\u001b[0m loadedModel\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrmsprop\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\garni\\Documents\\Oulu\\Stage\\beeHealth\\DeepLearning\\NeuralHivework\\ModelFunctions.py:103\u001b[0m, in \u001b[0;36mLoadJSONModel\u001b[1;34m(fileName, verbose)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mLoadJSONModel\u001b[39m(fileName : \u001b[39mstr\u001b[39m, verbose : \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     96\u001b[0m     \u001b[39m# Load a model from a json and a h5 file\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[39m# @arguments : - fileName : name of the file where the data is stored without the extension\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m \n\u001b[0;32m    102\u001b[0m     \u001b[39m# open the json file\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     jsonFile \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(fileName\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.json\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    104\u001b[0m     loadedModelJson \u001b[39m=\u001b[39m jsonFile\u001b[39m.\u001b[39mread()\n\u001b[0;32m    105\u001b[0m     jsonFile\u001b[39m.\u001b[39mclose()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ModelBeeHealth/model2.json'"
     ]
    }
   ],
   "source": [
    "p, y = model.predict(model.loadModel(f\"ModelBeeHealth/model2\"), X, Y, timeVerbose=True)\n",
    "\n",
    "s = 0\n",
    "for prediction, expected in zip(p.tolist(), y.tolist()) :\n",
    "    if(prediction[0] == (int) (expected)):\n",
    "        s += 1\n",
    "\n",
    "print(f\"Accuracy : {s / len(p) * 100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
